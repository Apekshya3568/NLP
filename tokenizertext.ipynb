{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99209213-4c27-48d2-a033-63da4c359375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'tokenizertext.ipynb', LF will be replaced by CRLF the next time Git touches it\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e76aaf-e4b6-4a74-93a0-22db54b7ff27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main a618c8c] Changes\n",
      " 3 files changed, 376 insertions(+), 4619 deletions(-)\n",
      " delete mode 100644 Untitled.ipynb\n",
      " delete mode 100644 tokenizer_texts.ipynb\n",
      " create mode 100644 tokenizertext.ipynb\n"
     ]
    }
   ],
   "source": [
    "!git commit -m\"Changes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7a01c3-8764-41ea-baf9-f01471f15b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/Apekshya3568/NLP.git\n",
      "   5cfbb2a..a618c8c  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cce0f5-736d-4346-ba99-7c88d53b55c2",
   "metadata": {},
   "source": [
    "Tokenize text using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257cce3-1f28-4363-994f-55529a47af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac8501c-5f29-40bb-8b6d-90818aefd920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'NLP', 'and', 'it', 'is', 'exciting', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"I am learning NLP and it is exciting!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad0a4d4-8ba3-4c78-ad76-fdfef16058a6",
   "metadata": {},
   "source": [
    "spaCy Lemma, POS, Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95dcde0e-2d37-469c-a31e-a5dc86f2cfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (3.8.11)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.21.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (70.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: wrapt in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca0eb98-b90d-4307-91eb-8f921c8cee72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - ------------------------------------- 0.5/12.8 MB 932.9 kB/s eta 0:00:14\n",
      "     - ------------------------------------- 0.5/12.8 MB 932.9 kB/s eta 0:00:14\n",
      "     -- ------------------------------------ 0.8/12.8 MB 987.4 kB/s eta 0:00:13\n",
      "     --- ----------------------------------- 1.0/12.8 MB 882.6 kB/s eta 0:00:14\n",
      "     --- ----------------------------------- 1.0/12.8 MB 882.6 kB/s eta 0:00:14\n",
      "     --- ----------------------------------- 1.0/12.8 MB 882.6 kB/s eta 0:00:14\n",
      "     --- ----------------------------------- 1.0/12.8 MB 882.6 kB/s eta 0:00:14\n",
      "     --- ----------------------------------- 1.0/12.8 MB 882.6 kB/s eta 0:00:14\n",
      "     --- ----------------------------------- 1.0/12.8 MB 882.6 kB/s eta 0:00:14\n",
      "     --- ----------------------------------- 1.3/12.8 MB 512.2 kB/s eta 0:00:23\n",
      "     --- ----------------------------------- 1.3/12.8 MB 512.2 kB/s eta 0:00:23\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 527.5 kB/s eta 0:00:22\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 527.5 kB/s eta 0:00:22\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 527.5 kB/s eta 0:00:22\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 527.5 kB/s eta 0:00:22\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 527.5 kB/s eta 0:00:22\n",
      "     ----- --------------------------------- 1.8/12.8 MB 428.3 kB/s eta 0:00:26\n",
      "     ----- --------------------------------- 1.8/12.8 MB 428.3 kB/s eta 0:00:26\n",
      "     ----- --------------------------------- 1.8/12.8 MB 428.3 kB/s eta 0:00:26\n",
      "     ------ -------------------------------- 2.1/12.8 MB 438.2 kB/s eta 0:00:25\n",
      "     ------ -------------------------------- 2.1/12.8 MB 438.2 kB/s eta 0:00:25\n",
      "     ------- ------------------------------- 2.4/12.8 MB 456.5 kB/s eta 0:00:23\n",
      "     ------- ------------------------------- 2.6/12.8 MB 477.8 kB/s eta 0:00:22\n",
      "     ------- ------------------------------- 2.6/12.8 MB 477.8 kB/s eta 0:00:22\n",
      "     -------- ------------------------------ 2.9/12.8 MB 494.9 kB/s eta 0:00:21\n",
      "     --------- ----------------------------- 3.1/12.8 MB 515.5 kB/s eta 0:00:19\n",
      "     --------- ----------------------------- 3.1/12.8 MB 515.5 kB/s eta 0:00:19\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 535.4 kB/s eta 0:00:18\n",
      "     ----------- --------------------------- 3.7/12.8 MB 554.9 kB/s eta 0:00:17\n",
      "     ----------- --------------------------- 3.7/12.8 MB 554.9 kB/s eta 0:00:17\n",
      "     ----------- --------------------------- 3.9/12.8 MB 561.9 kB/s eta 0:00:16\n",
      "     ----------- --------------------------- 3.9/12.8 MB 561.9 kB/s eta 0:00:16\n",
      "     ------------ -------------------------- 4.2/12.8 MB 565.5 kB/s eta 0:00:16\n",
      "     ------------ -------------------------- 4.2/12.8 MB 565.5 kB/s eta 0:00:16\n",
      "     ------------- ------------------------- 4.5/12.8 MB 568.7 kB/s eta 0:00:15\n",
      "     ------------- ------------------------- 4.5/12.8 MB 568.7 kB/s eta 0:00:15\n",
      "     -------------- ------------------------ 4.7/12.8 MB 571.6 kB/s eta 0:00:15\n",
      "     --------------- ----------------------- 5.0/12.8 MB 580.7 kB/s eta 0:00:14\n",
      "     --------------- ----------------------- 5.0/12.8 MB 580.7 kB/s eta 0:00:14\n",
      "     --------------- ----------------------- 5.2/12.8 MB 580.6 kB/s eta 0:00:14\n",
      "     --------------- ----------------------- 5.2/12.8 MB 580.6 kB/s eta 0:00:14\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 588.7 kB/s eta 0:00:13\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 588.7 kB/s eta 0:00:13\n",
      "     ----------------- --------------------- 5.8/12.8 MB 587.2 kB/s eta 0:00:12\n",
      "     ----------------- --------------------- 5.8/12.8 MB 587.2 kB/s eta 0:00:12\n",
      "     ----------------- --------------------- 5.8/12.8 MB 587.2 kB/s eta 0:00:12\n",
      "     ----------------- --------------------- 5.8/12.8 MB 587.2 kB/s eta 0:00:12\n",
      "     ------------------ -------------------- 6.0/12.8 MB 567.0 kB/s eta 0:00:12\n",
      "     ------------------ -------------------- 6.0/12.8 MB 567.0 kB/s eta 0:00:12\n",
      "     ------------------- ------------------- 6.3/12.8 MB 560.0 kB/s eta 0:00:12\n",
      "     ------------------- ------------------- 6.3/12.8 MB 560.0 kB/s eta 0:00:12\n",
      "     ------------------- ------------------- 6.3/12.8 MB 560.0 kB/s eta 0:00:12\n",
      "     ------------------- ------------------- 6.3/12.8 MB 560.0 kB/s eta 0:00:12\n",
      "     ------------------- ------------------- 6.6/12.8 MB 550.0 kB/s eta 0:00:12\n",
      "     ------------------- ------------------- 6.6/12.8 MB 550.0 kB/s eta 0:00:12\n",
      "     -------------------- ------------------ 6.8/12.8 MB 546.8 kB/s eta 0:00:11\n",
      "     -------------------- ------------------ 6.8/12.8 MB 546.8 kB/s eta 0:00:11\n",
      "     -------------------- ------------------ 6.8/12.8 MB 546.8 kB/s eta 0:00:11\n",
      "     --------------------- ----------------- 7.1/12.8 MB 540.5 kB/s eta 0:00:11\n",
      "     --------------------- ----------------- 7.1/12.8 MB 540.5 kB/s eta 0:00:11\n",
      "     --------------------- ----------------- 7.1/12.8 MB 540.5 kB/s eta 0:00:11\n",
      "     --------------------- ----------------- 7.1/12.8 MB 540.5 kB/s eta 0:00:11\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 528.0 kB/s eta 0:00:11\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 528.0 kB/s eta 0:00:11\n",
      "     ----------------------- --------------- 7.6/12.8 MB 529.0 kB/s eta 0:00:10\n",
      "     ----------------------- --------------- 7.6/12.8 MB 529.0 kB/s eta 0:00:10\n",
      "     ----------------------- --------------- 7.9/12.8 MB 527.1 kB/s eta 0:00:10\n",
      "     ----------------------- --------------- 7.9/12.8 MB 527.1 kB/s eta 0:00:10\n",
      "     ------------------------ -------------- 8.1/12.8 MB 530.9 kB/s eta 0:00:09\n",
      "     ------------------------ -------------- 8.1/12.8 MB 530.9 kB/s eta 0:00:09\n",
      "     ------------------------ -------------- 8.1/12.8 MB 530.9 kB/s eta 0:00:09\n",
      "     ------------------------- ------------- 8.4/12.8 MB 528.0 kB/s eta 0:00:09\n",
      "     -------------------------- ------------ 8.7/12.8 MB 534.7 kB/s eta 0:00:08\n",
      "     --------------------------- ----------- 8.9/12.8 MB 543.9 kB/s eta 0:00:08\n",
      "     --------------------------- ----------- 8.9/12.8 MB 543.9 kB/s eta 0:00:08\n",
      "     --------------------------- ----------- 9.2/12.8 MB 550.6 kB/s eta 0:00:07\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 558.7 kB/s eta 0:00:07\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 558.7 kB/s eta 0:00:07\n",
      "     ----------------------------- --------- 9.7/12.8 MB 554.6 kB/s eta 0:00:06\n",
      "     ----------------------------- --------- 9.7/12.8 MB 554.6 kB/s eta 0:00:06\n",
      "     ----------------------------- -------- 10.0/12.8 MB 561.3 kB/s eta 0:00:06\n",
      "     ----------------------------- -------- 10.0/12.8 MB 561.3 kB/s eta 0:00:06\n",
      "     ------------------------------ ------- 10.2/12.8 MB 560.2 kB/s eta 0:00:05\n",
      "     ------------------------------- ------ 10.5/12.8 MB 567.0 kB/s eta 0:00:05\n",
      "     -------------------------------- ----- 11.0/12.8 MB 586.9 kB/s eta 0:00:04\n",
      "     ---------------------------------- --- 11.5/12.8 MB 608.3 kB/s eta 0:00:03\n",
      "     ----------------------------------- -- 11.8/12.8 MB 618.3 kB/s eta 0:00:02\n",
      "     -------------------------------------  12.6/12.8 MB 649.5 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 649.5 kB/s eta 0:00:01\n",
      "     -------------------------------------- 12.8/12.8 MB 653.2 kB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6e1db3-f1bf-478a-bea0-56e5fd55dc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN\n",
      "is be AUX\n",
      "looking look VERB\n",
      "at at ADP\n",
      "buying buy VERB\n",
      "a a DET\n",
      "UK UK PROPN\n",
      "- - PUNCT\n",
      "based base VERB\n",
      "startup startup NOUN\n",
      "for for ADP\n",
      "$ $ SYM\n",
      "1 1 NUM\n",
      "billion billion NUM\n",
      ". . PUNCT\n",
      "\n",
      "Named Entities:\n",
      "Apple ORG\n",
      "UK GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Apple is looking at buying a UK-based startup for $1 billion.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_)\n",
    "\n",
    "print(\"\\nNamed Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eed7061-7d95-420c-b801-a917a740989d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love new nlp course soooo helpful\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()                                 # lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)                 # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()             # remove extra spaces\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    lemmas = [token.lemma_ for token in doc if token.lemma_ not in stop_words]\n",
    "    cleaned = ' '.join(lemmas)\n",
    "    return cleaned\n",
    "\n",
    "sample = \"I'm LOVING the new NLP course!!! It’s soooo helpful.\"\n",
    "print(clean_text(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df97a90-1bdb-4cfc-babe-c0fcfb2274c8",
   "metadata": {},
   "source": [
    "MINI-PROJECT #1 (DO THIS AFTER LEARNING)\n",
    "Build a Text Cleaning Pipeline\n",
    "Final pipeline should take raw text → output cleaned text.\n",
    "\n",
    "Full Steps:\n",
    "1. Input Raw Text:User enters any messy text.\n",
    "2. Process:Convert to lowercase,Remove URLs,Remove punctuation,Remove numbers,Remove special characters,Lemmatize,Remove stopwords\n",
    "3. Output:Clean, normalized text ready for:classification,topic modeling,vectorization,embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed902f72-d170-43d0-83ad-7dc56210a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text → nlp amazing visit info number\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def text_cleaning_pipeline(text):\n",
    "    text = text.lower() # lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text) #Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)#Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text) #Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() #Remove extra spaces\n",
    "    doc = nlp(text)\n",
    "\n",
    "    #Lemmatize and remove stopwords\n",
    "    cleaned_tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if token.lemma_ not in stop_words\n",
    "    ]\n",
    "\n",
    "    #Join back to string\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "\n",
    "# Test your pipeline\n",
    "raw = \"NLP is AMAZING!!! Visit https://example.com for more info. Numbers: 12345.\"\n",
    "print(\"Cleaned text →\", text_cleaning_pipeline(raw))\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1fb6bf-3f1b-4866-99b1-727fcafab02c",
   "metadata": {},
   "source": [
    "Extracting Emails\n",
    "\n",
    "Pattern:[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb79b292-0ab0-476c-bac5-3c3eb04ceea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['support@mail.com', 'admin123@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Contact us at support@mail.com or admin123@gmail.com\"\n",
    "emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
    "print(emails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b81401-a0a8-4d2d-a9f1-beeba42c7b6b",
   "metadata": {},
   "source": [
    "Extracting Phone Numbers\n",
    "\n",
    "Nepali Format Examples:9841234567,+977-9841234567,01-4234567\n",
    "Pattern (flexible):\n",
    "\n",
    "(\\+977[- ]?)?\\d{2,4}[- ]?\\d{6,7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37802bea-0c37-4ff2-a96d-78becdba4e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9843447364', '+977-9876453213', '01-4444898']\n"
     ]
    }
   ],
   "source": [
    "text = \"9843447364 , +977-9876453213 , 01-4444898\"\n",
    "phones = re.findall(r\"(?:\\+977[- ]?)?\\d{2,4}[- ]?\\d{6,7}\", text)\n",
    "print(phones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a1fb9-c77d-4f28-8af0-fd825e82cbdb",
   "metadata": {},
   "source": [
    "Extracting Names (Simple)\n",
    "\n",
    "Basic pattern: Word starting with capital letter\n",
    "\n",
    "[A-Z][a-z]+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dd844c2-5efb-48a9-8106-02184e9c75b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apekshya', 'Lamichhane', 'Aakanchhya', 'Lamichhane', 'Shiva']\n"
     ]
    }
   ],
   "source": [
    "text = \"Apekshya Lamichhane , Aakanchhya Lamichhane , Shiva \"\n",
    "names = re.findall(r\"[A-Z][a-z]+\",text)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b3048af-4d6f-46a0-987b-3373d673d525",
   "metadata": {},
   "source": [
    "TEXT VALIDATION\n",
    "\n",
    "Validation means: text must match the pattern.\n",
    "\n",
    "Examples:\n",
    "\n",
    "✔ Validate Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38a2182-8440-422a-b08c-5daa39ec48b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\"\n",
    "bool(re.match(pattern, \"hello@gmail.com\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9247e-d1d6-4b89-a4ad-aa811f2d9073",
   "metadata": {},
   "source": [
    "Validate Password\n",
    "At least 8 characters\n",
    "One uppercase\n",
    "One lowercase\n",
    "One digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21a6a3c6-d768-4c00-a10c-6f9f6687075a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d).{8,}$\"\n",
    "bool(re.match(pattern, \"peopLe@12\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb329247-42db-4c0f-b8a6-1336207a696a",
   "metadata": {},
   "source": [
    "— Extract Dates\n",
    "Support these formats:\n",
    "2025-01-10\n",
    "10/01/2025\n",
    "Jan 10, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c87b04-0ef0-4683-9427-a121487ae4fc",
   "metadata": {},
   "source": [
    "(\\d{4}-\\d{2}-\\d{2})|(\\d{2}/\\d{2}/\\d{4})|([A-Za-z]{3,9} \\d{1,2}, \\d{4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76edd756-5cd3-49c4-88bf-530cfb04512d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2025-01-10', '', ''), ('', '10/01/2025', ''), ('', '', 'Jan 10, 2025')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Events: 2025-01-10, 10/01/2025, Jan 10, 2025\"\n",
    "dates = re.findall(r\"(\\d{4}-\\d{2}-\\d{2})|(\\d{2}/\\d{2}/\\d{4})|([A-Za-z]{3,9} \\d{1,2}, \\d{4})\", text)\n",
    "print(dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0e3de-2616-4f2b-abd7-417c2c2910ca",
   "metadata": {},
   "source": [
    "Clean a Paragraph Using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3207c60c-e1e3-485d-bf9b-0b516af01227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey This is a big paragraph with numbers URLs hashtags fun NLP and emails test mail com\n"
     ]
    }
   ],
   "source": [
    "para = \"\"\"\n",
    "Hey!!! This   is a big paragraph with!!! \n",
    "numbers 12345, URLs https://google.com, \n",
    "hashtags #fun #NLP2025, and emails test@mail.com\n",
    "\"\"\"\n",
    "\n",
    "cleaned = re.sub(r\"http\\S+\", \"\", para)       # remove URLs\n",
    "cleaned = re.sub(r\"\\d+\", \"\", cleaned)        # remove numbers\n",
    "cleaned = re.sub(r\"[^\\w\\s]\", \" \", cleaned)   # remove punctuation\n",
    "cleaned = re.sub(r\"\\s+\", \" \", cleaned)       # fix spaces\n",
    "\n",
    "print(cleaned.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc70166-ab72-4604-9166-40f49f88a673",
   "metadata": {},
   "source": [
    "Regex-based Log File Analyzer\n",
    "\n",
    "This is your full project for Regex learning.\n",
    "\n",
    "You will build a tool that:\n",
    "Reads a log file\n",
    "Extracts errors, warnings, timestamps, IP addresses\n",
    "Shows summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58a563b4-1c42-464a-8586-2d0310afb70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_logs': 4,\n",
       " 'timestamps': ['2025-01-10 10:22:34',\n",
       "  '2025-01-10 10:23:01',\n",
       "  '2025-01-10 10:24:10',\n",
       "  '2025-01-10 10:25:45'],\n",
       " 'ip_addresses': ['192.168.1.10', '192.168.1.11', '10.0.0.1'],\n",
       " 'errors': ['ERROR Login failed for user 192.168.1.10',\n",
       "  'ERROR Database connection lost at 10.0.0.1'],\n",
       " 'warnings': ['WARNING Disk almost full']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def analyze_log(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    timestamps = re.findall(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\", data)\n",
    "    ips = re.findall(r\"\\d+\\.\\d+\\.\\d+\\.\\d+\", data)\n",
    "    errors = re.findall(r\"ERROR.*\", data)\n",
    "    warnings = re.findall(r\"WARNING.*\", data)\n",
    "    \n",
    "    return {\n",
    "        \"total_logs\": len(data.splitlines()),\n",
    "        \"timestamps\": timestamps,\n",
    "        \"ip_addresses\": ips,\n",
    "        \"errors\": errors,\n",
    "        \"warnings\": warnings\n",
    "    }\n",
    "\n",
    "result = analyze_log(\"test.txt\")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3ce353-759e-4abf-b0c4-b8738b78bec4",
   "metadata": {},
   "source": [
    "POS Tagging Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59db4a35-76bc-4888-a1f8-dc9e939d8552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\PC/nltk_data'\n    - 'C:\\\\Users\\\\PC\\\\OneDrive\\\\Desktop\\\\NLP-CODE\\\\myvenv\\\\nltk_data'\n    - 'C:\\\\Users\\\\PC\\\\OneDrive\\\\Desktop\\\\NLP-CODE\\\\myvenv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\PC\\\\OneDrive\\\\Desktop\\\\NLP-CODE\\\\myvenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\PC\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33maveraged_perceptron_tagger\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mApple is looking at buying a UK-based startup for $1 billion.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m pos_tags = pos_tag(tokens)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(pos_tags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\NLP-CODE\\myvenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\NLP-CODE\\myvenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\NLP-CODE\\myvenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\NLP-CODE\\myvenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\NLP-CODE\\myvenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\NLP-CODE\\myvenv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\PC/nltk_data'\n    - 'C:\\\\Users\\\\PC\\\\OneDrive\\\\Desktop\\\\NLP-CODE\\\\myvenv\\\\nltk_data'\n    - 'C:\\\\Users\\\\PC\\\\OneDrive\\\\Desktop\\\\NLP-CODE\\\\myvenv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\PC\\\\OneDrive\\\\Desktop\\\\NLP-CODE\\\\myvenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\PC\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"Apple is looking at buying a UK-based startup for $1 billion.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c233c8-a5dc-4513-8580-8cce83b9f7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
